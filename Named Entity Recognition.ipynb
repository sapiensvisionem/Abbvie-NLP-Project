{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifications:\n",
    "\n",
    "1. Take dataframe containing content, title columns\n",
    "2. Output dataframe appended with new column called country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exceptions:\n",
    "\n",
    "1. If dataframe does not have content or title then output error with print warning\n",
    "2. If articles do not contain geopolitical entities, then it outputs NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure:\n",
    "    \n",
    "1. Machine learning model is saved outside \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jihunlee/opt/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Dataframe for Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "article_list = original data\n",
    "df = data frame to be used for machine learning\n",
    "row = each geopolitical entity\n",
    "column = features to be tested for importance in statistical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jihunlee/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  import sys\n",
      "/Users/jihunlee/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  \n",
      "/Users/jihunlee/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "state_country = pd.read_csv(\"/Users/jihunlee/Desktop/state.csv\", encoding='latin-1')\n",
    "country_codes = pd.read_csv(\"/Users/jihunlee/Desktop/country-codes.csv\", encoding='latin-1')\n",
    "city_codes1 = pd.read_csv(\"/Users/jihunlee/Desktop/city1.csv\", encoding='latin-1')\n",
    "city_codes2 = pd.read_csv(\"/Users/jihunlee/Desktop/city2.csv\", encoding='latin-1')\n",
    "city_codes3 = pd.read_csv(\"/Users/jihunlee/Desktop/city3.csv\", encoding='latin-1')\n",
    "city_codes = pd.concat([city_codes1, city_codes2, city_codes3])\n",
    "state_country = state_country.set_index('State').T.to_dict('list')\n",
    "city_country = city_codes.set_index('city').T.to_dict('list')\n",
    "country_codes = country_codes.set_index('abbreviation').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def us_or_uk(abbrev):\n",
    "    if abbrev in ['US','USA','United States']:\n",
    "        return('United States of America')\n",
    "    elif abbrev in ['UK']:\n",
    "        return('United Kingdom')\n",
    "    else:\n",
    "        return(abbrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_country(location):\n",
    "    if location != None: # location could be None\n",
    "        state = state_country.get(location) # map state to country\n",
    "        city = city_country.get(location) # map city to country\n",
    "        if state != None: # if location can be found in state\n",
    "            state = [x for x in state if str(x) !='nan'][0] # get rid of nan's and choose the first element\n",
    "            country = country_codes.get(''.join(state))\n",
    "        elif city != None:\n",
    "            city = [x for x in city if str(x) not in ['nan','!']][0] # get rid of nan's and choose the first element\n",
    "            country = country_codes.get(''.join(city))\n",
    "        else:\n",
    "            country = us_or_uk(location)\n",
    "        return(''.join(country))\n",
    "    else: \n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_country2(location):\n",
    "    if location != None: # location could be None\n",
    "        state = state_country.get(location) # map state to country\n",
    "        city = city_country.get(location) # map city to country\n",
    "        if state != None: # if location can be found in state\n",
    "            state = [x for x in state if str(x) !='nan'][0] # get rid of nan's and choose the first element\n",
    "            country = country_codes.get(''.join(state))\n",
    "        elif city != None:\n",
    "            city = [x for x in city if str(x) not in ['nan','!']][0] # get rid of nan's and choose the first element\n",
    "            country = country_codes.get(''.join(city))\n",
    "        else:\n",
    "            country = us_or_uk(location)\n",
    "        return(''.join(country))\n",
    "    else: \n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_title(location, title):\n",
    "    if location in title:\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an exmaple DataFrame \n",
    "article = pd.DataFrame({'Title':['China is Attacked'], \n",
    "                        'Content':['Japan attacks Beijing with airstrike and kills 500 people.']}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_dataframe2(article):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # article has Content column and Country column\n",
    "    content = article.Content\n",
    "    title = article.Title\n",
    "    result = pd.DataFrame()\n",
    "    doc = nlp(str(content))\n",
    "\n",
    "    # create an array of all entities\n",
    "    a = [ent for ent in doc.ents]\n",
    "    if sum([ent.label_ == 'GPE' for ent in a]) > 0:\n",
    "\n",
    "        a = [str(ent) for ent in doc.ents]\n",
    "\n",
    "        # create an array of all entity labels\n",
    "        b = [str(ent.label_) for ent in doc.ents]\n",
    "\n",
    "        # create an array of all entity positions\n",
    "        p = [int(ent.start_char) for ent in doc.ents]\n",
    "\n",
    "        # create a dataframe out of these two \n",
    "        temp = pd.DataFrame({'entity':a,'labels':b,'position':p})\n",
    "\n",
    "        # subset rows whose labels are geopolitical entities\n",
    "        temp = temp[temp.labels == 'GPE']\n",
    "        temp = temp.drop(columns = ['labels'])\n",
    "        temp['entity'] = list(map(convert_to_country2, temp.entity)) # convert all cities and states into countries\n",
    "\n",
    "        # Group rows by entities, get counts, and get the first of each location count (creating three columns: entity, count, and first location)\n",
    "        temp1 = temp.groupby(['entity']).count().reset_index().rename(columns={'position':'freq'})\n",
    "        temp2 = temp.groupby(['entity'])['position'].first().reset_index()\n",
    "        # Join these two tables to produce a table containing entities, count, and location\n",
    "        df = temp1.merge(temp2, on = 'entity', how = 'inner')\n",
    "\n",
    "        # d is a variable measuring the proportion of this frequency\n",
    "        # higher d means it was mentioned more frequently in the article\n",
    "        d = df.freq\n",
    "        all_count = sum(d)\n",
    "        d = [value/all_count for value in d]\n",
    "\n",
    "        # e is an array of start character position\n",
    "        # f is an array of binary values whose 1 is if it is the first geopolitical entity mentioned, and 0 if not\n",
    "        m = len(df)\n",
    "        f = [0]*m\n",
    "        min_index = np.argmin(df.position)\n",
    "        f[min_index] = 1\n",
    "\n",
    "        # g is an array of binary values whose 1 is if it is the last geopolitical entity mentioned, and 0 if not\n",
    "        g = [0]*m\n",
    "        max_index = np.argmax(df.position)\n",
    "        g[max_index] = 1\n",
    "\n",
    "        # h is an array of binary values whose 1 is if it is the most frequently mentioned entity, and 0 if not\n",
    "        h = [0]*m\n",
    "        most_index = np.argmax(df.count)\n",
    "        h[most_index] = 1\n",
    "\n",
    "        # x is an array of binary values whose 1 is if entity is also in the title, else 0\n",
    "        x = [in_title(x, title) for x  in df.entity]\n",
    "\n",
    "        # append columns\n",
    "        df['proportion'] = d\n",
    "        df['first'] = f\n",
    "        df['last'] = g\n",
    "        df['mode'] = h\n",
    "        df['title'] = x\n",
    "\n",
    "        # drop the position column\n",
    "        df = df.drop(columns=['position'])\n",
    "    else: \n",
    "        df = pd.DataFrame(np.array([None, None, None, None, None, None, None]).reshape(-1,7),\n",
    "                         columns=['entity','freq','proportion','first','last','mode','title'])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>freq</th>\n",
       "      <th>proportion</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>mode</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>China</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Japan</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity  freq  proportion  first  last  mode  title\n",
       "0  China     1         0.5      0     1     1      0\n",
       "1  Japan     1         0.5      1     0     0      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_dataframe2(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_dataframe(df):\n",
    "    df = df[['proportion','first','last','mode','title']]\n",
    "    df = df.astype(float)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jihunlee/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:61: FutureWarning: \n",
      "The current behaviour of 'Series.argmin' is deprecated, use 'idxmin'\n",
      "instead.\n",
      "The behavior of 'argmin' will be corrected to return the positional\n",
      "minimum in the future. For now, use 'series.values.argmin' or\n",
      "'np.argmin(np.array(values))' to get the position of the minimum\n",
      "row.\n",
      "  return bound(*args, **kwds)\n",
      "/Users/jihunlee/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:61: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "df2 = sklearn_dataframe(ner_dataframe2(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('/Users/jihunlee/Desktop/rf_location_extraction.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = pd.read_csv(\"/Users/jihunlee/Desktop/country-codes.csv\", encoding='latin-1')\n",
    "country_codes2 = country_codes.set_index('country').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_proba(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94658861, 0.05341139],\n",
       "       [0.98280003, 0.01719997]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_index = np.argmax(predictions[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jihunlee/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:61: FutureWarning: \n",
      "The current behaviour of 'Series.argmin' is deprecated, use 'idxmin'\n",
      "instead.\n",
      "The behavior of 'argmin' will be corrected to return the positional\n",
      "minimum in the future. For now, use 'series.values.argmin' or\n",
      "'np.argmin(np.array(values))' to get the position of the minimum\n",
      "row.\n",
      "  return bound(*args, **kwds)\n",
      "/Users/jihunlee/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:61: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "df = ner_dataframe2(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_extraction(model, article):\n",
    "    model = joblib.load(model)\n",
    "    df = ner_dataframe2(article)\n",
    "    df2 = sklearn_dataframe(df)\n",
    "    predictions = model.predict_proba(df2)\n",
    "    location_index = location_index = np.argmax(predictions[:,0])\n",
    "    location = df.entity[location_index]\n",
    "    country_codes2.get(''.join(location))\n",
    "    print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_dataframe(article_list):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # article_list has content column, Country column\n",
    "    content_list = article_list.Content\n",
    "    title_list = article_list.Title\n",
    "    location_list = article_list.Country\n",
    "    n = len(article_list)\n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        title = title_list[i]\n",
    "        content = content_list[i]\n",
    "        correct_location = location_list[i]\n",
    "        doc = nlp(content)\n",
    "        \n",
    "        # create an array of all entities\n",
    "        a = [ent for ent in doc.ents]\n",
    "        if sum([ent.label_ == 'GPE' for ent in a]) > 0:\n",
    "            \n",
    "            a = [str(ent) for ent in doc.ents]\n",
    "            \n",
    "            # create an array of all entity labels\n",
    "            b = [str(ent.label_) for ent in doc.ents]\n",
    "\n",
    "            # create an array of all entity positions\n",
    "            p = [int(ent.start_char) for ent in doc.ents]\n",
    "\n",
    "            # create a dataframe out of these two \n",
    "            temp = pd.DataFrame({'entity':a,'labels':b,'position':p})\n",
    "\n",
    "            # subset rows whose labels are geopolitical entities\n",
    "            temp = temp[temp.labels == 'GPE']\n",
    "            temp = temp.drop(columns = ['labels'])\n",
    "            temp['entity'] = list(map(convert_to_country, temp.entity)) # convert all cities and states into countries\n",
    "\n",
    "            # Group rows by entities, get counts, and get the first of each location count (creating three columns: entity, count, and first location)\n",
    "            temp1 = temp.groupby(['entity']).count().reset_index().rename(columns={'position':'freq'})\n",
    "            temp2 = temp.groupby(['entity'])['position'].first().reset_index()\n",
    "            # Join these two tables to produce a table containing entities, count, and location\n",
    "            df = temp1.merge(temp2, on = 'entity', how = 'inner')\n",
    "\n",
    "            # c is the response variable: 1 if this geopolitical entity is the correct location, 0 if not\n",
    "            c = [location == correct_location.title() for location in df.entity] # .title() for capitalizing the first letter\n",
    "\n",
    "            # append the column\n",
    "            df['response'] = c\n",
    "\n",
    "            # d is a variable measuring the proportion of this frequency\n",
    "            # higher d means it was mentioned more frequently in the article\n",
    "            d = df.freq\n",
    "            all_count = sum(d)\n",
    "            d = [value/all_count for value in d]\n",
    "\n",
    "            # e is an array of start character position\n",
    "            # f is an array of binary values whose 1 is if it is the first geopolitical entity mentioned, and 0 if not\n",
    "            m = len(df)\n",
    "            f = [0]*m\n",
    "            min_index = np.argmin(df.position)\n",
    "            f[min_index] = 1\n",
    "\n",
    "            # g is an array of binary values whose 1 is if it is the last geopolitical entity mentioned, and 0 if not\n",
    "            g = [0]*m\n",
    "            max_index = np.argmax(df.position)\n",
    "            g[max_index] = 1\n",
    "\n",
    "            # h is an array of binary values whose 1 is if it is the most frequently mentioned entity, and 0 if not\n",
    "            h = [0]*m\n",
    "            most_index = np.argmax(df.count)\n",
    "            h[most_index] = 1\n",
    "\n",
    "            # x is an array of binary values whose 1 is if entity is also in the title, else 0\n",
    "            x = [in_title(x, title) for x  in df.entity]\n",
    "\n",
    "            # append columns\n",
    "            df['proportion'] = d\n",
    "            df['first'] = f\n",
    "            df['last'] = g\n",
    "            df['mode'] = h\n",
    "            df['title'] = x\n",
    "\n",
    "            # drop the position column\n",
    "            df = df.drop(columns=['position'])\n",
    "        else: \n",
    "            df = pd.DataFrame(np.array([None, None, None, None, None, None, None, None]).reshape(-1,8),\n",
    "                             columns=['entity','freq','response','proportion','first','last','mode','title'])\n",
    "        # concatenate with previous dataframe by rows\n",
    "        result = pd.concat([result,df])\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "train = pd.read_csv('/Users/jihunlee/Desktop/NER_Train.csv')\n",
    "# Subset country only\n",
    "train = train[~pd.isna(train.Country)&~pd.isna(train.Title)].reset_index()\n",
    "train = train.iloc[range(200)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = ner_dataframe(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_df[['proportion','first','last','mode','title']], train_df['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(map(lambda x: 1 if x else 0, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proportion</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>mode</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.039801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.012579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>921 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    proportion  first  last  mode  title\n",
       "2     0.500000    1.0   0.0   0.0    0.0\n",
       "2     0.040000    0.0   0.0   0.0    0.0\n",
       "3     0.045455    0.0   0.0   0.0    0.0\n",
       "4     0.040000    0.0   0.0   0.0    0.0\n",
       "0     0.006944    0.0   1.0   1.0    1.0\n",
       "..         ...    ...   ...   ...    ...\n",
       "11    0.039801    0.0   0.0   0.0    0.0\n",
       "3     0.012579    0.0   0.0   0.0    0.0\n",
       "10    0.013889    0.0   0.0   0.0    0.0\n",
       "16    0.010000    0.0   0.0   0.0    0.0\n",
       "0     1.000000    1.0   1.0   1.0    0.0\n",
       "\n",
       "[921 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [1,3,5,7],\n",
    "    'max_features': [3,4,5],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [2,4,6,8],\n",
    "    'n_estimators': [500, 1000, 1500]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 10, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 432 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:   25.5s\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 989 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1434 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1961 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2568 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3257 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4320 out of 4320 | elapsed: 12.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 5,\n",
       " 'max_features': 5,\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 1000}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9468354430379747\n",
      "0.48545375597047336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "best_grid_rf = grid_search.best_estimator_\n",
    "rf_predict = best_grid_rf.predict(X_test)\n",
    "print(accuracy_score(y_test, rf_predict))\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "print(cohen_kappa_score(rf_predict, y_test, labels=None, weights=None, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [1,2,3, 4, 5]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n",
    "                    silent=True, nthread=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=params, scoring='roc_auc', n_jobs=4, cv=skf.split(X,Y), verbose=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_xgb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid_xgb = grid_search_xgb.best_estimator_\n",
    "xgb_predict = best_grid_xgb.predict(X_test)\n",
    "accuracy_score(y_test, xgb_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib \n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(best_grid_rf, '/Users/jihunlee/Desktop/rf_location_extraction.pkl') \n",
    "# Load the model from the file \n",
    "rf_mod = joblib.load('/Users/jihunlee/Desktop/rf_location_extraction.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = train_df[['proportion','first','last','mode','title']], train_df['response']\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "pred = clf.predict(X)\n",
    "clf.predict_proba(X)\n",
    "clf.score(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X, y).predict(X)\n",
    "accuracy_score(y_pred, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(X,y)\n",
    "y_pred2 = clf.predict(X)\n",
    "accuracy_score(y,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(y_pred, y, labels=None, weights=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
