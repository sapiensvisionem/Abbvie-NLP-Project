{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jihunlee/opt/anaconda3/lib/python3.7/site-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n",
      "/Users/jihunlee/opt/anaconda3/lib/python3.7/site-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping\n",
      "/Users/jihunlee/opt/anaconda3/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "/Users/jihunlee/opt/anaconda3/lib/python3.7/site-packages/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence, defaultdict\n",
      "/Users/jihunlee/opt/anaconda3/lib/python3.7/site-packages/botocore/awsrequest.py:624: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  class HeadersDict(collections.MutableMapping):\n",
      "/Users/jihunlee/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis.gensim\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus  \n",
    "from nltk.text import Text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content #\n",
    "1. Import and inspect data \n",
    "2. Preprocessing: tfidf tdm, tokenization, lemmatization, stemming\n",
    "3. Machine Learning Classification\n",
    "    - Logistic Regression\n",
    "    - SVM\n",
    "    - Discriminant\n",
    "    - Naive Bayes\n",
    "    - Random Forest\n",
    "    - Boosting\n",
    "    - Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORT and INSPECT DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Level</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Publish_Date</th>\n",
       "      <th>URL</th>\n",
       "      <th>Scrape_Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>Content</th>\n",
       "      <th>...</th>\n",
       "      <th>article_neg</th>\n",
       "      <th>article_neu</th>\n",
       "      <th>article_pos</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>extracted_gpes</th>\n",
       "      <th>loc1</th>\n",
       "      <th>loc2</th>\n",
       "      <th>loc3</th>\n",
       "      <th>GRE_list</th>\n",
       "      <th>Content_GRE_com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Man confesses to cyberattack on German politic...</td>\n",
       "      <td>A 20-year-old man arrested in connection with ...</td>\n",
       "      <td>1/8/19 0:00</td>\n",
       "      <td>https://www.cnn.com/2019/01/08/europe/arrest-g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Man confesses to cyberattack on German politic...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>[\"A 20-year-old man arrested in connection wit...</td>\n",
       "      <td>['Germany', 'Wiesbaden', 'Hesse', 'Frankfurt',...</td>\n",
       "      <td>GERMANY</td>\n",
       "      <td>WIESBADEN</td>\n",
       "      <td>HESSE</td>\n",
       "      <td>['Germany', 'Wiesbaden', 'Hesse', 'Frankfurt']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Israeli</td>\n",
       "      <td>Anonymous Arab' cyberattacks hit Israel</td>\n",
       "      <td>Several Israeli government websites appeared t...</td>\n",
       "      <td>4/7/13 0:00</td>\n",
       "      <td>https://www.cnn.com/2013/04/07/world/meast/isr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Anonymous Arab' cyberattacks hit Israel</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['Several Israeli government websites appeared...</td>\n",
       "      <td>['Israel', 'Israel', 'Yisrael', 'Israel']</td>\n",
       "      <td>ISRAEL</td>\n",
       "      <td>YISRAEL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Israel', 'Yisrael']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>Pentagon says China using cyberattacks</td>\n",
       "      <td>The Pentagon has accused China of trying to ex...</td>\n",
       "      <td>5/7/13 0:00</td>\n",
       "      <td>https://money.cnn.com/2013/05/07/news/china-cy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Pentagon says China using cyberattacks</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['The Pentagon has accused China of trying to ...</td>\n",
       "      <td>['China', 'U.S.', 'U.S.', 'U.S.', 'China', 'U....</td>\n",
       "      <td>CHINA</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>IRAN</td>\n",
       "      <td>['China', 'U.S.', 'Iran', 'Virginia', 'Shanghai']</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>World</td>\n",
       "      <td>New startups prime targets for cyberattacks</td>\n",
       "      <td>Startups take note: Cybercriminals are onto yo...</td>\n",
       "      <td>5/23/13 0:00</td>\n",
       "      <td>https://money.cnn.com/2013/05/23/technology/st...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CNN</td>\n",
       "      <td>New startups prime targets for cyberattacks</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['Startups take note: Cybercriminals are onto ...</td>\n",
       "      <td>['Cybercrime', 'Startups']</td>\n",
       "      <td>CYBERCRIME</td>\n",
       "      <td>STARTUPS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Cybercrime', 'Startups']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>World</td>\n",
       "      <td>Cybercrime's easiest prey: Small businesses</td>\n",
       "      <td>Small businesses are the 'most victimized' Cyb...</td>\n",
       "      <td>4/23/13 0:00</td>\n",
       "      <td>https://money.cnn.com/2013/04/22/smallbusiness...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Cybercrime's easiest prey: Small businesses</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.412</td>\n",
       "      <td>[\"Small businesses are the 'most victimized' C...</td>\n",
       "      <td>['Verizon']</td>\n",
       "      <td>VERIZON</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Verizon']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Level   Entity                                              Title  \\\n",
       "0     0    0.0  Germany  Man confesses to cyberattack on German politic...   \n",
       "1     0    0.0  Israeli            Anonymous Arab' cyberattacks hit Israel   \n",
       "2     0    0.0      USA             Pentagon says China using cyberattacks   \n",
       "3     0    0.0    World        New startups prime targets for cyberattacks   \n",
       "4     0    0.0    World        Cybercrime's easiest prey: Small businesses   \n",
       "\n",
       "                                                Text  Publish_Date  \\\n",
       "0  A 20-year-old man arrested in connection with ...   1/8/19 0:00   \n",
       "1  Several Israeli government websites appeared t...   4/7/13 0:00   \n",
       "2  The Pentagon has accused China of trying to ex...   5/7/13 0:00   \n",
       "3  Startups take note: Cybercriminals are onto yo...  5/23/13 0:00   \n",
       "4  Small businesses are the 'most victimized' Cyb...  4/23/13 0:00   \n",
       "\n",
       "                                                 URL Scrape_Date Source  \\\n",
       "0  https://www.cnn.com/2019/01/08/europe/arrest-g...         NaN    CNN   \n",
       "1  https://www.cnn.com/2013/04/07/world/meast/isr...         NaN    CNN   \n",
       "2  https://money.cnn.com/2013/05/07/news/china-cy...         NaN    CNN   \n",
       "3  https://money.cnn.com/2013/05/23/technology/st...         NaN    CNN   \n",
       "4  https://money.cnn.com/2013/04/22/smallbusiness...         NaN    CNN   \n",
       "\n",
       "                                             Content  ... article_neg  \\\n",
       "0  Man confesses to cyberattack on German politic...  ...         0.0   \n",
       "1            Anonymous Arab' cyberattacks hit Israel  ...         0.0   \n",
       "2             Pentagon says China using cyberattacks  ...         0.0   \n",
       "3        New startups prime targets for cyberattacks  ...         0.0   \n",
       "4        Cybercrime's easiest prey: Small businesses  ...         0.0   \n",
       "\n",
       "   article_neu  article_pos  \\\n",
       "0        1.000        0.000   \n",
       "1        1.000        0.000   \n",
       "2        1.000        0.000   \n",
       "3        1.000        0.000   \n",
       "4        0.588        0.412   \n",
       "\n",
       "                                     tokenized_sents  \\\n",
       "0  [\"A 20-year-old man arrested in connection wit...   \n",
       "1  ['Several Israeli government websites appeared...   \n",
       "2  ['The Pentagon has accused China of trying to ...   \n",
       "3  ['Startups take note: Cybercriminals are onto ...   \n",
       "4  [\"Small businesses are the 'most victimized' C...   \n",
       "\n",
       "                                      extracted_gpes        loc1       loc2  \\\n",
       "0  ['Germany', 'Wiesbaden', 'Hesse', 'Frankfurt',...     GERMANY  WIESBADEN   \n",
       "1          ['Israel', 'Israel', 'Yisrael', 'Israel']      ISRAEL    YISRAEL   \n",
       "2  ['China', 'U.S.', 'U.S.', 'U.S.', 'China', 'U....       CHINA       U.S.   \n",
       "3                         ['Cybercrime', 'Startups']  CYBERCRIME   STARTUPS   \n",
       "4                                        ['Verizon']     VERIZON        NaN   \n",
       "\n",
       "    loc3                                           GRE_list Content_GRE_com  \n",
       "0  HESSE     ['Germany', 'Wiesbaden', 'Hesse', 'Frankfurt']               1  \n",
       "1    NaN                              ['Israel', 'Yisrael']               1  \n",
       "2   IRAN  ['China', 'U.S.', 'Iran', 'Virginia', 'Shanghai']               2  \n",
       "3    NaN                         ['Cybercrime', 'Startups']               0  \n",
       "4    NaN                                        ['Verizon']               0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#articles = pd.read_csv(\"New Labled Data2.csv\")\n",
    "articles = pd.read_csv(\"/Users/jihunlee/Desktop/fulldata_complete_GPR_v2.csv\")\n",
    "articles.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREPROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove special characters using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_text (text):\n",
    "    text = text.str.lower() # lowercase\n",
    "    text = text.str.replace(r\"\\#\",\"\") # replaces hashtags\n",
    "    text = text.str.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n",
    "    text = text.str.replace(r\"@\",\"\")\n",
    "    text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \")\n",
    "    text = text.str.replace(\"\\s{2,}\", \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text\"]=normalise_text(train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['Content_clean'] = articles['Text'].map(lambda x: re.sub('[^a-zA-Z0-9 @ . , : - _]', '', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    A 20yearold man arrested in connection with a data breach that affected thousands of people in Germany has confessed to police, the countrys federal prosecutor, Georg Ungefuk, told reporters at a ...\n",
       "1    Several Israeli government websites appeared to crash as antiIsraeli hackers launched cyberattacks Sunday, but Israeli hackers also claimed their own victory. The website that promoted the OpIsrae...\n",
       "2    The Pentagon has accused China of trying to extract sensitive information from U.S. government computers, the latest in a series of rhetorical skirmishes between the two countries on the issue of ...\n",
       "3    Startups take note: Cybercriminals are onto you. Cyberattackers can sniff out new businesses to target as quickly as two months after they come into existence, according to a new report from cyber...\n",
       "4    Small businesses are the most victimized Cybercriminals have picked their easiest prey: Small businesses. A data breach investigations report from Verizon VZ, released Tuesday, showed that small b...\n",
       "Name: Content_clean, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "articles['Content_clean'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop missing values in the three columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(articles[['Level','Title','Content_clean']])\n",
    "df = df.dropna(subset=['Level','Title','Content_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer on Article Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Content_clean']\n",
    "y = df['Level']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.7,random_state=10)\n",
    "vect = CountVectorizer()\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Naive Bayes Model and print evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score is: 0.8424710424710424\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.97      0.93       984\n",
      "         1.0       0.40      0.26      0.31       132\n",
      "         2.0       0.70      0.60      0.64       179\n",
      "\n",
      "    accuracy                           0.84      1295\n",
      "   macro avg       0.66      0.61      0.63      1295\n",
      "weighted avg       0.82      0.84      0.83      1295\n",
      "\n",
      "[[950  21  13]\n",
      " [ 65  34  33]\n",
      " [ 41  31 107]]\n"
     ]
    }
   ],
   "source": [
    "# strictly unigram model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "print(\"accuracy score is: \" + str(metrics.accuracy_score(y_test, y_pred_class)))\n",
    "print(classification_report(y_test, y_pred_class))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.]\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB test1\n",
    "test_text=\"china has earthquake\"\n",
    "test_text=[test_text]\n",
    "test_text_dtm = vect.transform(test_text)\n",
    "y_pred_class = nb.predict(test_text_dtm)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB test2\n",
    "test_text=\"taylor swift has a new song called earthquake\"\n",
    "test_text=[test_text]\n",
    "test_text_dtm = vect.transform(test_text)\n",
    "y_pred_class = nb.predict(test_text_dtm)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.]\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB test3\n",
    "test_text=\"England has flood,1000 people died\"\n",
    "test_text=[test_text]\n",
    "test_text_dtm = vect.transform(test_text)\n",
    "y_pred_class = nb.predict(test_text_dtm)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB test4\n",
    "test_text=\"England has flood, no death\"\n",
    "test_text=[test_text]\n",
    "test_text_dtm = vect.transform(test_text)\n",
    "y_pred_class = nb.predict(test_text_dtm)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB test5\n",
    "test_text=\"there is a bomb in iran, no death\"\n",
    "test_text=[test_text]\n",
    "test_text_dtm = vect.transform(test_text)\n",
    "y_pred_class = nb.predict(test_text_dtm)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB test6\n",
    "test_text=\"there is a bomb in iran, causing 100 people death\"\n",
    "test_text=[test_text]\n",
    "test_text_dtm = vect.transform(test_text)\n",
    "y_pred_class = nb.predict(test_text_dtm)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB test7\n",
    "test_text=\"china and united states have trade war\"\n",
    "test_text=[test_text]\n",
    "test_text_dtm = vect.transform(test_text)\n",
    "y_pred_class = nb.predict(test_text_dtm)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB test8\n",
    "test_text=\"There is a war in iron, causing 1000 death\"\n",
    "test_text=[test_text]\n",
    "test_text_dtm = vect.transform(test_text)\n",
    "y_pred_class = nb.predict(test_text_dtm)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF on Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train) \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.31 ms, sys: 1.32 ms, total: 6.63 ms\n",
      "Wall time: 5.82 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7872340425531915"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "X_new_counts = count_vect.transform(X_test)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "y_pred_class = nb.predict(X_new_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "print(classification_report(y_test, y_pred_class))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))\n",
    "predicted = nb.predict(X_new_tfidf)\n",
    "np.mean(predicted ==  y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    " ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "     'vect__ngram_range': [(1, 1), (1, 2), (1, 3)], # \n",
    "     'tfidf__use_idf': (True, False),\n",
    "     'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8439716312056738"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text_clf.fit(X_train,y_train)\n",
    "#Pipeline(...)\n",
    "predicted = gs_clf.predict(X_test)\n",
    "np.mean(predicted ==  y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to know what the best parameters are?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to know what the best variables are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8581560283687943"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                            max_iter=5, tol=None)),\n",
    " ])\n",
    "\n",
    "text_clf.fit(X_train,y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted ==  y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "     'tfidf__use_idf': (True, False),\n",
    "     'C': [1, 10, 100, 1000], \n",
    "     'kernel': ['linear']},\n",
    "    {'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "     'tfidf__use_idf': (True, False),\n",
    "     'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8652482269503546"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = gs_clf.predict(X_test)\n",
    "np.mean(predicted ==  y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8652482269503546\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.99      0.94       111\n",
      "         1.0       0.38      0.23      0.29        13\n",
      "         2.0       0.90      0.53      0.67        17\n",
      "\n",
      "    accuracy                           0.87       141\n",
      "   macro avg       0.72      0.58      0.63       141\n",
      "weighted avg       0.85      0.87      0.85       141\n",
      "\n",
      "[[110   1   0]\n",
      " [  9   3   1]\n",
      " [  4   4   9]]\n"
     ]
    }
   ],
   "source": [
    "##X_new_counts = count_vect.transform(X_test)\n",
    "#X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "y_pred_class =gs_clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "print(classification_report(y_test, y_pred_class))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.]\n"
     ]
    }
   ],
   "source": [
    "# SVM test1\n",
    "test_text=\"china has earthquake\"\n",
    "test_text=[test_text]\n",
    "#test_text_dtm = vect.transform(test_text)\n",
    "y_pred_class = text_clf.predict(test_text)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "# SVM test2\n",
    "test_text=\"taylor swift has a new song called earthquake\"\n",
    "test_text=[test_text]\n",
    "#test_text_dtm = vect.transform(test_text)\n",
    "y_pred_class = text_clf.predict(test_text)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countvectorizer = CountVectorizer(stop_words='english')\n",
    "#countvectorizer = CountVectorizer(max_df=0.5)\n",
    "#countvectorizer = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words='english', strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabular...\n",
       "                ('clf',\n",
       "                 SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=5, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=42,\n",
       "                               shuffle=True, tol=None, validation_fraction=0.1,\n",
       "                               verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer(stop_words='english')),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                            max_iter=5, tol=None)),\n",
    " ])\n",
    "\n",
    "parameters = {\n",
    "     'vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "     'vect__max_df': (0.1,0.2,0.5),\n",
    "     'vect__min_df': (2,3),\n",
    "     'tfidf__use_idf': (True, False)\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "predicted = gs_clf.predict(X_test)\n",
    "np.mean(predicted ==  y_test)\n",
    "\n",
    "text_clf.fit(X_train,y_train)\n",
    "#Pipeline(...)\n",
    "#predicted = text_clf.predict(X_test)\n",
    "#np.mean(predicted ==  y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8723404255319149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.97      0.94       111\n",
      "         1.0       0.57      0.31      0.40        13\n",
      "         2.0       0.73      0.65      0.69        17\n",
      "\n",
      "    accuracy                           0.87       141\n",
      "   macro avg       0.74      0.64      0.68       141\n",
      "weighted avg       0.86      0.87      0.86       141\n",
      "\n",
      "[[108   0   3]\n",
      " [  8   4   1]\n",
      " [  3   3  11]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_class =gs_clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "print(classification_report(y_test, y_pred_class))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8723404255319149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.97      0.94       111\n",
      "         1.0       0.57      0.31      0.40        13\n",
      "         2.0       0.73      0.65      0.69        17\n",
      "\n",
      "    accuracy                           0.87       141\n",
      "   macro avg       0.74      0.64      0.68       141\n",
      "weighted avg       0.86      0.87      0.86       141\n",
      "\n",
      "[[108   0   3]\n",
      " [  8   4   1]\n",
      " [  3   3  11]]\n",
      "[[108   0   3]\n",
      " [  8   4   1]\n",
      " [  3   3  11]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_class =gs_clf.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "print(classification_report(y_test, y_pred_class))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Testing as following--\n",
    "Word N-grams, \n",
    "char n-grams，\n",
    "char_wb grams，\n",
    "Controlling features in CountVectorizer，\n",
    "Sentiment Score as predictors,\n",
    "Time as feature, \n",
    "Title combined with articles,\n",
    "Other machine learning method,\n",
    "(Feature Selections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dig in N-grams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import metrics\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<141x469046 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 210949 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf = df\n",
    "X = subdf['Content_clean']\n",
    "y = subdf.Level\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=10)\n",
    "vect =  CountVectorizer(analyzer='word', ngram_range=(1,3))\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.9 ms, sys: 26.3 ms, total: 78.3 ms\n",
      "Wall time: 68.2 ms\n",
      "0.851063829787234\n",
      "[[110   0   1]\n",
      " [  9   3   1]\n",
      " [  3   7   7]]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Model\n",
    "nb = MultinomialNB()\n",
    "%time nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "print(classification_report(y_test, y_pred_class))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8502673796791443"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer(analyzer='word', ngram_range=(1,3))),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    " ])\n",
    "\n",
    "#text_clf.fit(X_train,y_train)\n",
    "#Pipeline(...)\n",
    "#predicted = text_clf.predict(X_test)\n",
    "#np.mean(predicted ==  y_test)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "     'vect__ngram_range': [(1, 1), (1, 2),(1,3)],\n",
    "     'tfidf__use_idf': (True, False),\n",
    "     'clf__alpha': (1e-2, 1e-3),\n",
    "     }\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train,y_train)\n",
    "gs_clf.best_score_\n",
    "#gs_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlling features in CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 999 µs, sys: 3 µs, total: 1 ms\n",
      "Wall time: 1.01 ms\n",
      "0.5390070921985816\n",
      "[[58 35 18]\n",
      " [ 2  4  7]\n",
      " [ 1  2 14]]\n"
     ]
    }
   ],
   "source": [
    "subdf = df\n",
    "X = subdf['Content_clean']\n",
    "y = subdf.Level\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=10)\n",
    "vect =  CountVectorizer(lowercase=False, stop_words='english',\n",
    "                                  max_df=0.8, min_df=0.2, max_features=1000, ngram_range=(1,3))\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm\n",
    "\n",
    "# Naive Bayes Model\n",
    "nb = MultinomialNB()\n",
    "%time nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "#print(classification_report(y_test, y_pred_class))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.77 ms, sys: 2.4 ms, total: 10.2 ms\n",
      "Wall time: 9.86 ms\n",
      "0.574468085106383\n",
      "[[63 43  5]\n",
      " [ 2  8  3]\n",
      " [ 0  7 10]]\n"
     ]
    }
   ],
   "source": [
    "subdf = df\n",
    "X = subdf['Content_clean']\n",
    "y = subdf.Level\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=10)\n",
    "vect = countvectorizer = CountVectorizer(analyzer='char', ngram_range=(1,3))\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm\n",
    "\n",
    "# Naive Bayes Model\n",
    "nb = MultinomialNB()\n",
    "%time nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "#print(classification_report(y_test, y_pred_class))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "char_wb grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.68 ms, sys: 2.03 ms, total: 8.71 ms\n",
      "Wall time: 8.71 ms\n",
      "0.6028368794326241\n",
      "[[67 40  4]\n",
      " [ 2  8  3]\n",
      " [ 0  7 10]]\n"
     ]
    }
   ],
   "source": [
    "subdf = df\n",
    "X = subdf['Content_clean']\n",
    "y = subdf.Level\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=10)\n",
    "vect = countvectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(1,3))\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm\n",
    "\n",
    "# Naive Bayes Model\n",
    "nb = MultinomialNB()\n",
    "%time nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "#print(classification_report(y_test, y_pred_class))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
